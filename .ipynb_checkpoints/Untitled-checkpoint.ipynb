{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from os import listdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from keras.applications import VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.utils import to_categorical, plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import dump\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.applications.vgg16 import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.merge import add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-4403fc350bd4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mdirectory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Flickr_Data/Flickr Images/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-24-4403fc350bd4>\u001b[0m in \u001b[0;36mextract_features\u001b[0;34m(dir)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mimage\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mpreprocess_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mfeature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mimage_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1165\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1166\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1167\u001b[0;31m                                             steps=steps)\n\u001b[0m\u001b[1;32m   1168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1169\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2664\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2666\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2667\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2668\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2635\u001b[0m                                 session)\n\u001b[0;32m-> 2636\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2637\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def extract_features(dir):\n",
    "    model = VGG16()\n",
    "    \n",
    "    model.layers.pop()\n",
    "    model = Model(inputs=model.inputs, outputs = model.layers[-1].output)\n",
    "    features = dict()\n",
    "    \n",
    "    for name in listdir(dir):\n",
    "        filename = dir+'/'+name\n",
    "        image = load_img(filename, target_size=(224,224))\n",
    "        image = img_to_array(image)\n",
    "        image = image.reshape((1, image.shape[0], image.shape[1],image.shape[2]))\n",
    "        image= preprocess_input(image)\n",
    "        feature = model.predict(image)\n",
    "        \n",
    "        image_id=name.split('.')[0]\n",
    "        features[image_id]=feature\n",
    "    return features\n",
    "directory = 'Flickr_Data/Flickr Images/'\n",
    "features = extract_features(directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(features, open('features.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def read_doc(file):\n",
    "    infile = open(file,'r')\n",
    "    text = infile.read()\n",
    "    infile.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'Flickr_Data/Flickr Text Data/Flickr8k.token.txt'\n",
    "# load descriptions\n",
    "doc = read_doc(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract descriptions for images\n",
    "def load_descriptions(doc):\n",
    "\tmapping = dict()\n",
    "\t# process lines\n",
    "\tfor line in doc.split('\\n'):\n",
    "\t\t# split line by white space\n",
    "\t\ttokens = line.split()\n",
    "\t\tif len(line) < 2:\n",
    "\t\t\tcontinue\n",
    "\t\t# take the first token as the image id, the rest as the description\n",
    "\t\timage_id, image_desc = tokens[0], tokens[1:]\n",
    "\t\t# remove filename from image id\n",
    "\t\timage_id = image_id.split('.')[0]\n",
    "\t\t# convert description tokens back to string\n",
    "\t\timage_desc = ' '.join(image_desc)\n",
    "\t\t# create the list if needed\n",
    "\t\tif image_id not in mapping:\n",
    "\t\t\tmapping[image_id] = list()\n",
    "\t\t# store description\n",
    "\t\tmapping[image_id].append(image_desc)\n",
    "\treturn mapping\n",
    "# parse descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(desc):\n",
    "    table = str.maketrans('','',string.punctuation)\n",
    "    for key, desc_list in desc.items():\n",
    "        for i in range(len(desc_list)):\n",
    "            d = desc_list[i]\n",
    "            d=d.split()\n",
    "            d = [j.lower() for j in d]\n",
    "            d = [j.translate(table) for j in d]\n",
    "            d = [j for j in d if len(j)>1]\n",
    "            d = [j for j in d if j.isalpha()]\n",
    "            desc_list=''.join(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_vocab(desc):\n",
    "    all_desc=set()\n",
    "    for key in desc.keys():\n",
    "        [all_desc.update(d.split()) for d in desc[key]]\n",
    "    return all_desc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_desc(desc, file):\n",
    "    lines = list()\n",
    "    for key, desc_list in desc.items():\n",
    "        for desc in desc_list:\n",
    "            lines.append(key+' '+desc)\n",
    "            \n",
    "    data = '\\n'.join(lines)\n",
    "    \n",
    "    with open(file,'w') as outfile:\n",
    "        outfile.write(data)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: 8092 \n",
      "Vocabulary Size: 8763\n"
     ]
    }
   ],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'r')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text\n",
    " \n",
    "# extract descriptions for images\n",
    "def load_descriptions(doc):\n",
    "\tmapping = dict()\n",
    "\t# process lines\n",
    "\tfor line in doc.split('\\n'):\n",
    "\t\t# split line by white space\n",
    "\t\ttokens = line.split()\n",
    "\t\tif len(line) < 2:\n",
    "\t\t\tcontinue\n",
    "\t\t# take the first token as the image id, the rest as the description\n",
    "\t\timage_id, image_desc = tokens[0], tokens[1:]\n",
    "\t\t# remove filename from image id\n",
    "\t\timage_id = image_id.split('.')[0]\n",
    "\t\t# convert description tokens back to string\n",
    "\t\timage_desc = ' '.join(image_desc)\n",
    "\t\t# create the list if needed\n",
    "\t\tif image_id not in mapping:\n",
    "\t\t\tmapping[image_id] = list()\n",
    "\t\t# store description\n",
    "\t\tmapping[image_id].append(image_desc)\n",
    "\treturn mapping\n",
    " \n",
    "def clean_descriptions(descriptions):\n",
    "\t# prepare translation table for removing punctuation\n",
    "\ttable = str.maketrans('', '', string.punctuation)\n",
    "\tfor key, desc_list in descriptions.items():\n",
    "\t\tfor i in range(len(desc_list)):\n",
    "\t\t\tdesc = desc_list[i]\n",
    "\t\t\t# tokenize\n",
    "\t\t\tdesc = desc.split()\n",
    "\t\t\t# convert to lower case\n",
    "\t\t\tdesc = [word.lower() for word in desc]\n",
    "\t\t\t# remove punctuation from each token\n",
    "\t\t\tdesc = [w.translate(table) for w in desc]\n",
    "\t\t\t# remove hanging 's' and 'a'\n",
    "\t\t\tdesc = [word for word in desc if len(word)>1]\n",
    "\t\t\t# remove tokens with numbers in them\n",
    "\t\t\tdesc = [word for word in desc if word.isalpha()]\n",
    "\t\t\t# store as string\n",
    "\t\t\tdesc_list[i] =  ' '.join(desc)\n",
    "            \n",
    "\n",
    "def to_vocabulary(descriptions):\n",
    "\t# build a list of all description strings\n",
    "\tall_desc = set()\n",
    "\tfor key in descriptions.keys():\n",
    "\t\t[all_desc.update(d.split()) for d in descriptions[key]]\n",
    "\treturn all_desc\n",
    " \n",
    "# save descriptions to file, one per line\n",
    "def save_descriptions(descriptions, filename):\n",
    "\tlines = list()\n",
    "\tfor key, desc_list in descriptions.items():\n",
    "\t\tfor desc in desc_list:\n",
    "\t\t\tlines.append(key + ' ' + desc)\n",
    "\tdata = '\\n'.join(lines)\n",
    "\tfile = open(filename, 'w')\n",
    "\tfile.write(data)\n",
    "\tfile.close()\n",
    " \n",
    "filename = 'Flickr_Data/Flickr Text Data/Flickr8k.token.txt'\n",
    "# load descriptions\n",
    "doc = load_doc(filename)\n",
    "# parse descriptions\n",
    "descriptions = load_descriptions(doc)\n",
    "print('Loaded: %d ' % len(descriptions))\n",
    "# clean descriptions\n",
    "clean_descriptions(descriptions)\n",
    "# summarize vocabulary\n",
    "vocabulary = to_vocabulary(descriptions)\n",
    "print('Vocabulary Size: %d' % len(vocabulary))\n",
    "# save to file\n",
    "save_descriptions(descriptions, 'descriptions.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file):\n",
    "    \n",
    "    with open(file,'r') as infile:\n",
    "        txt = infile.read()\n",
    "    return txt\n",
    "\n",
    "\n",
    "def load_set(file):\n",
    "    doc=load_data(file)\n",
    "    dataset=list()\n",
    "    \n",
    "    for line in doc.split('\\n'):\n",
    "        if len(line)<1:\n",
    "            continue\n",
    "        identifier = line.split('.')[0]\n",
    "        dataset.append(identifier)\n",
    "        \n",
    "        \n",
    "    return set(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clean(file, dataset):\n",
    "    # load document\n",
    "    doc = load_doc(filename)\n",
    "    descriptions = dict()\n",
    "    for line in doc.split('\\n'):\n",
    "        # split line by white space\n",
    "        tokens = line.split()\n",
    "        # split id from description\n",
    "        image_id, image_desc = tokens[0], tokens[1:]\n",
    "        # skip images not in the set\n",
    "        if image_id in dataset:\n",
    "        # create list\n",
    "            if image_id not in descriptions:\n",
    "                descriptions[image_id] = list()\n",
    "            # wrap description in tokens\n",
    "            desc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
    "            # store\n",
    "            descriptions[image_id].append(desc)\n",
    "    return descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_features(file, dataset):\n",
    "    feat = load(open(file, 'b'))\n",
    "    features = {k:feat[i] for i in dataset}\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 6000\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-d193190b98f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Dataset: %d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# descriptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain_descriptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_clean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'descriptions.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Descriptions: train=%d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_descriptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# photo features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-38-3da91563652d>\u001b[0m in \u001b[0;36mload_clean\u001b[0;34m(file, dataset)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;31m# split id from description\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mimage_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_desc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;31m# skip images not in the set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mimage_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "filename = 'Flickr_Data/Flickr Text Data/Flickr_8k.trainImages.txt'\n",
    "train = load_set(filename)\n",
    "print('Dataset: %d' % len(train))\n",
    "# descriptions\n",
    "train_descriptions = load_clean('descriptions.txt', train)\n",
    "print('Descriptions: train=%d' % len(train_descriptions))\n",
    "# photo features\n",
    "train_features = load_photo_features('features.pkl', train)\n",
    "print('Photos: train=%d' % len(train_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 6000\n",
      "Descriptions: train=6000\n"
     ]
    }
   ],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'r')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text\n",
    " \n",
    "# load a pre-defined list of photo identifiers\n",
    "def load_set(filename):\n",
    "\tdoc = load_doc(filename)\n",
    "\tdataset = list()\n",
    "\t# process line by line\n",
    "\tfor line in doc.split('\\n'):\n",
    "\t\t# skip empty lines\n",
    "\t\tif len(line) < 1:\n",
    "\t\t\tcontinue\n",
    "\t\t# get the image identifier\n",
    "\t\tidentifier = line.split('.')[0]\n",
    "\t\tdataset.append(identifier)\n",
    "\treturn set(dataset)\n",
    " \n",
    "# load clean descriptions into memory\n",
    "def load_clean_descriptions(filename, dataset):\n",
    "\t# load document\n",
    "\tdoc = load_doc(filename)\n",
    "\tdescriptions = dict()\n",
    "\tfor line in doc.split('\\n'):\n",
    "\t\t# split line by white space\n",
    "\t\ttokens = line.split()\n",
    "\t\t# split id from description\n",
    "\t\timage_id, image_desc = tokens[0], tokens[1:]\n",
    "\t\t# skip images not in the set\n",
    "\t\tif image_id in dataset:\n",
    "\t\t\t# create list\n",
    "\t\t\tif image_id not in descriptions:\n",
    "\t\t\t\tdescriptions[image_id] = list()\n",
    "\t\t\t# wrap description in tokens\n",
    "\t\t\tdesc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
    "\t\t\t# store\n",
    "\t\t\tdescriptions[image_id].append(desc)\n",
    "\treturn descriptions\n",
    " \n",
    "# load photo features\n",
    "def load_photo_features(filename, dataset):\n",
    "\t# load all features\n",
    "\tall_features = load(open(filename, 'rb'))\n",
    "\t# filter features\n",
    "\tfeatures = {k: all_features[k] for k in dataset}\n",
    "\treturn features\n",
    " \n",
    "# load training dataset (6K)\n",
    "filename = 'Flickr_Data/Flickr Text Data//Flickr_8k.trainImages.txt'\n",
    "train = load_set(filename)\n",
    "print('Dataset: %d' % len(train))\n",
    "# descriptions\n",
    "train_descriptions = load_clean_descriptions('descriptions.txt', train)\n",
    "print('Descriptions: train=%d' % len(train_descriptions))\n",
    "# photo features\n",
    "train_features = load_photo_features('features.pkl', train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_line(descriptions):\n",
    "    all_desc=list()\n",
    "    for key in descriptions.keys():\n",
    "        [all_desc.append(d) for d in descriptions[key]]\n",
    "        \n",
    "    return all_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_token(descriptions):\n",
    "    lines = to_line(descriptions)\n",
    "    \n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = create_token(train_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(tokenizer, max_len, descriptions, photos):\n",
    "    X1, X2, y= list(),list(),list()\n",
    "    \n",
    "    for key, desc_list in descriptions.items():\n",
    "        for desc in desc_list:\n",
    "            seq = tokenizer.texts_to_sequences([desc])[0]\n",
    "            for i in range(1, len(seq)):\n",
    "                \n",
    "                in_seq, out_seq=seq[:i], seq[i]\n",
    "                \n",
    "                in_seq=pad_sequences([in_seq], max_len)[0]\n",
    "                out_seq=to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "                X1.append(photos[key][0])\n",
    "                X2.append(in_seq)\n",
    "                y.append(out_seq)\n",
    "        return array(X1), array(X2), array(y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lines(descriptions):\n",
    "\tall_desc = list()\n",
    "\tfor key in descriptions.keys():\n",
    "\t\t[all_desc.append(d) for d in descriptions[key]]\n",
    "\treturn all_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_len(descriptions):\n",
    "    lines = to_lines(descriptions)\n",
    "    return max(len(d.split()) for d in lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'Flickr_Data/Flickr Text Data/Flickr_8k.devImages.txt'\n",
    "test=load_set(filename) \n",
    "test_descriptions = load_clean_descriptions('descriptions.txt', test)\n",
    "max_length = max_len(train_descriptions)\n",
    "test_features = load_photo_features('features.pkl', test)\n",
    "X1train, X2train, ytrain = create_sequences(tokenizer, max_length, train_descriptions, train_features)\n",
    "X1test, X2test, ytest = create_sequences(tokenizer, max_length, test_descriptions, test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Dropout, LSTM, Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(vocab_size, max_len):\n",
    "    inp1 = Input(shape = (4096,))\n",
    "    f1=Dropout(0.5)(inp1)\n",
    "    f2=Dense(256, activation='relu')(f1)\n",
    "    \n",
    "    inp2=Input(shape=(max_len,))\n",
    "    se1 = Embedding(vocab_size, 256, mask_zero=True)(inp2)\n",
    "    se2=Dropout(0.5)(se1)\n",
    "    se3 = LSTM(256)(se2)\n",
    "    \n",
    "    decoder = add([f2,se3])\n",
    "    d2 = Dense(256, activation='relu')(decoder)\n",
    "    \n",
    "    out = Dense(vocab_size, activation='softmax')(d2)\n",
    "    \n",
    "    model = Model(inputs = [inp1,inp2], outputs = out)\n",
    "    model.compile(loss= 'categorical_crossentropy', optimizer='adam')\n",
    "    \n",
    "    print(model.summary())\n",
    "    \n",
    "    #plot_model(model, to_file='model.png', show_shapes=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = 'model-ep{epoch:03d}-loss{loss:.3f}-val_loss={val_loss:.3f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only = True, mode='min')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_22 (InputLayer)           (None, 34)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_21 (InputLayer)           (None, 4096)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, 34, 256)      1940224     input_22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 4096)         0           input_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 34, 256)      0           embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 256)          1048832     dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_5 (LSTM)                   (None, 256)          525312      dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 256)          0           dense_13[0][0]                   \n",
      "                                                                 lstm_5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 256)          65792       add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 7579)         1947803     dense_14[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 5,527,963\n",
      "Trainable params: 5,527,963\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = define_model(vocab_size, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 47 samples, validate on 67 samples\n",
      "Epoch 1/100\n",
      " - 1s - loss: 2.8979 - val_loss: 8.0454\n",
      "\n",
      "Epoch 00001: val_loss improved from 8.05662 to 8.04538, saving model to model-ep001-loss2.898-val_loss=8.045.h5\n",
      "Epoch 2/100\n",
      " - 1s - loss: 2.8909 - val_loss: 8.0385\n",
      "\n",
      "Epoch 00002: val_loss improved from 8.04538 to 8.03855, saving model to model-ep002-loss2.891-val_loss=8.039.h5\n",
      "Epoch 3/100\n",
      " - 1s - loss: 2.8432 - val_loss: 8.0391\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 8.03855\n",
      "Epoch 4/100\n",
      " - 1s - loss: 2.8144 - val_loss: 8.0348\n",
      "\n",
      "Epoch 00004: val_loss improved from 8.03855 to 8.03483, saving model to model-ep004-loss2.814-val_loss=8.035.h5\n",
      "Epoch 5/100\n",
      " - 1s - loss: 2.7501 - val_loss: 8.0358\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 8.03483\n",
      "Epoch 6/100\n",
      " - 0s - loss: 2.7097 - val_loss: 8.0342\n",
      "\n",
      "Epoch 00006: val_loss improved from 8.03483 to 8.03424, saving model to model-ep006-loss2.710-val_loss=8.034.h5\n",
      "Epoch 7/100\n",
      " - 1s - loss: 2.6349 - val_loss: 8.0418\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 8.03424\n",
      "Epoch 8/100\n",
      " - 1s - loss: 2.6381 - val_loss: 8.0667\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 8.03424\n",
      "Epoch 9/100\n",
      " - 1s - loss: 2.5887 - val_loss: 8.0905\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 8.03424\n",
      "Epoch 10/100\n",
      " - 1s - loss: 2.5362 - val_loss: 8.1201\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 8.03424\n",
      "Epoch 11/100\n",
      " - 1s - loss: 2.4684 - val_loss: 8.1525\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 8.03424\n",
      "Epoch 12/100\n",
      " - 1s - loss: 2.4496 - val_loss: 8.1678\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 8.03424\n",
      "Epoch 13/100\n",
      " - 1s - loss: 2.3926 - val_loss: 8.1710\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 8.03424\n",
      "Epoch 14/100\n",
      " - 1s - loss: 2.3860 - val_loss: 8.1600\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 8.03424\n",
      "Epoch 15/100\n",
      " - 1s - loss: 2.2993 - val_loss: 8.1460\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 8.03424\n",
      "Epoch 16/100\n",
      " - 1s - loss: 2.2663 - val_loss: 8.1511\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 8.03424\n",
      "Epoch 17/100\n",
      " - 1s - loss: 2.2108 - val_loss: 8.1778\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 8.03424\n",
      "Epoch 18/100\n",
      " - 1s - loss: 2.1343 - val_loss: 8.1974\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 8.03424\n",
      "Epoch 19/100\n",
      " - 1s - loss: 2.1562 - val_loss: 8.2124\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 8.03424\n",
      "Epoch 20/100\n",
      " - 1s - loss: 2.1215 - val_loss: 8.2415\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 8.03424\n",
      "Epoch 21/100\n",
      " - 1s - loss: 1.9916 - val_loss: 8.2872\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 8.03424\n",
      "Epoch 22/100\n",
      " - 1s - loss: 2.0089 - val_loss: 8.3647\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 8.03424\n",
      "Epoch 23/100\n",
      " - 1s - loss: 1.9561 - val_loss: 8.4161\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 8.03424\n",
      "Epoch 24/100\n",
      " - 1s - loss: 1.9761 - val_loss: 8.4410\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 8.03424\n",
      "Epoch 25/100\n",
      " - 1s - loss: 1.9397 - val_loss: 8.4993\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 8.03424\n",
      "Epoch 26/100\n",
      " - 1s - loss: 1.9033 - val_loss: 8.5315\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 8.03424\n",
      "Epoch 27/100\n",
      " - 1s - loss: 1.8201 - val_loss: 8.6298\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 8.03424\n",
      "Epoch 28/100\n",
      " - 1s - loss: 1.8492 - val_loss: 8.6535\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 8.03424\n",
      "Epoch 29/100\n",
      " - 1s - loss: 1.7415 - val_loss: 8.7071\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 8.03424\n",
      "Epoch 30/100\n",
      " - 1s - loss: 1.7431 - val_loss: 8.7642\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 8.03424\n",
      "Epoch 31/100\n",
      " - 1s - loss: 1.7527 - val_loss: 8.7805\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 8.03424\n",
      "Epoch 32/100\n",
      " - 1s - loss: 1.6650 - val_loss: 8.8632\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 8.03424\n",
      "Epoch 33/100\n",
      " - 1s - loss: 1.6423 - val_loss: 8.9444\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 8.03424\n",
      "Epoch 34/100\n",
      " - 1s - loss: 1.6609 - val_loss: 8.9452\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 8.03424\n",
      "Epoch 35/100\n",
      " - 1s - loss: 1.6438 - val_loss: 8.9973\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 8.03424\n",
      "Epoch 36/100\n",
      " - 1s - loss: 1.5578 - val_loss: 9.0648\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 8.03424\n",
      "Epoch 37/100\n",
      " - 1s - loss: 1.6034 - val_loss: 9.0583\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 8.03424\n",
      "Epoch 38/100\n",
      " - 1s - loss: 1.5297 - val_loss: 9.0110\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 8.03424\n",
      "Epoch 39/100\n",
      " - 1s - loss: 1.5739 - val_loss: 9.0596\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 8.03424\n",
      "Epoch 40/100\n",
      " - 1s - loss: 1.5160 - val_loss: 9.1912\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 8.03424\n",
      "Epoch 41/100\n",
      " - 1s - loss: 1.4904 - val_loss: 9.1873\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 8.03424\n",
      "Epoch 42/100\n",
      " - 1s - loss: 1.4447 - val_loss: 9.1376\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 8.03424\n",
      "Epoch 43/100\n",
      " - 1s - loss: 1.3933 - val_loss: 9.1980\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 8.03424\n",
      "Epoch 44/100\n",
      " - 1s - loss: 1.3888 - val_loss: 9.3146\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 8.03424\n",
      "Epoch 45/100\n",
      " - 1s - loss: 1.3635 - val_loss: 9.4275\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 8.03424\n",
      "Epoch 46/100\n",
      " - 1s - loss: 1.3814 - val_loss: 9.4678\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 8.03424\n",
      "Epoch 47/100\n",
      " - 1s - loss: 1.3533 - val_loss: 9.3833\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 8.03424\n",
      "Epoch 48/100\n",
      " - 1s - loss: 1.3044 - val_loss: 9.3183\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 8.03424\n",
      "Epoch 49/100\n",
      " - 1s - loss: 1.3262 - val_loss: 9.3302\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 8.03424\n",
      "Epoch 50/100\n",
      " - 1s - loss: 1.1673 - val_loss: 9.4536\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 8.03424\n",
      "Epoch 51/100\n",
      " - 1s - loss: 1.1839 - val_loss: 9.6979\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 8.03424\n",
      "Epoch 52/100\n",
      " - 1s - loss: 1.1922 - val_loss: 9.8810\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 8.03424\n",
      "Epoch 53/100\n",
      " - 1s - loss: 1.1967 - val_loss: 9.9011\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 8.03424\n",
      "Epoch 54/100\n",
      " - 1s - loss: 1.2149 - val_loss: 9.7208\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 8.03424\n",
      "Epoch 55/100\n",
      " - 1s - loss: 1.1446 - val_loss: 9.6474\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 8.03424\n",
      "Epoch 56/100\n",
      " - 1s - loss: 1.0843 - val_loss: 9.6719\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 8.03424\n",
      "Epoch 57/100\n",
      " - 1s - loss: 1.0967 - val_loss: 9.7779\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 8.03424\n",
      "Epoch 58/100\n",
      " - 1s - loss: 1.0728 - val_loss: 9.9427\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 8.03424\n",
      "Epoch 59/100\n",
      " - 1s - loss: 1.0238 - val_loss: 10.1156\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 8.03424\n",
      "Epoch 60/100\n",
      " - 1s - loss: 0.9907 - val_loss: 10.2871\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 8.03424\n",
      "Epoch 61/100\n",
      " - 1s - loss: 0.9963 - val_loss: 10.3363\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 8.03424\n",
      "Epoch 62/100\n",
      " - 1s - loss: 0.9943 - val_loss: 10.2408\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 8.03424\n",
      "Epoch 63/100\n",
      " - 1s - loss: 0.9459 - val_loss: 10.1438\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 8.03424\n",
      "Epoch 64/100\n",
      " - 1s - loss: 0.9148 - val_loss: 10.0575\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 8.03424\n",
      "Epoch 65/100\n",
      " - 1s - loss: 0.9174 - val_loss: 10.0393\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 8.03424\n",
      "Epoch 66/100\n",
      " - 1s - loss: 0.8588 - val_loss: 10.0459\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 8.03424\n",
      "Epoch 67/100\n",
      " - 1s - loss: 0.8749 - val_loss: 10.2579\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 8.03424\n",
      "Epoch 68/100\n",
      " - 1s - loss: 0.8436 - val_loss: 10.4823\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 8.03424\n",
      "Epoch 69/100\n",
      " - 1s - loss: 0.8284 - val_loss: 10.6981\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 8.03424\n",
      "Epoch 70/100\n",
      " - 1s - loss: 0.7961 - val_loss: 10.8235\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 8.03424\n",
      "Epoch 71/100\n",
      " - 1s - loss: 0.8025 - val_loss: 10.8751\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 8.03424\n",
      "Epoch 72/100\n",
      " - 1s - loss: 0.7484 - val_loss: 10.9061\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 8.03424\n",
      "Epoch 73/100\n",
      " - 1s - loss: 0.7997 - val_loss: 10.9395\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 8.03424\n",
      "Epoch 74/100\n",
      " - 1s - loss: 0.7924 - val_loss: 10.9010\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 8.03424\n",
      "Epoch 75/100\n",
      " - 1s - loss: 0.7294 - val_loss: 10.8043\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 8.03424\n",
      "Epoch 76/100\n",
      " - 1s - loss: 0.7297 - val_loss: 10.7683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00076: val_loss did not improve from 8.03424\n",
      "Epoch 77/100\n",
      " - 1s - loss: 0.6907 - val_loss: 10.9552\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 8.03424\n",
      "Epoch 78/100\n",
      " - 1s - loss: 0.6709 - val_loss: 11.0715\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 8.03424\n",
      "Epoch 79/100\n",
      " - 1s - loss: 0.6505 - val_loss: 11.1767\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 8.03424\n",
      "Epoch 80/100\n",
      " - 1s - loss: 0.6831 - val_loss: 11.2564\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 8.03424\n",
      "Epoch 81/100\n",
      " - 1s - loss: 0.7045 - val_loss: 11.3493\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 8.03424\n",
      "Epoch 82/100\n",
      " - 1s - loss: 0.6155 - val_loss: 11.4083\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 8.03424\n",
      "Epoch 83/100\n",
      " - 1s - loss: 0.6858 - val_loss: 11.2857\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 8.03424\n",
      "Epoch 84/100\n",
      " - 1s - loss: 0.6076 - val_loss: 11.1004\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 8.03424\n",
      "Epoch 85/100\n",
      " - 1s - loss: 0.6013 - val_loss: 10.9461\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 8.03424\n",
      "Epoch 86/100\n",
      " - 1s - loss: 0.5934 - val_loss: 11.1731\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 8.03424\n",
      "Epoch 87/100\n",
      " - 1s - loss: 0.5937 - val_loss: 11.3834\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 8.03424\n",
      "Epoch 88/100\n",
      " - 1s - loss: 0.5670 - val_loss: 11.4290\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 8.03424\n",
      "Epoch 89/100\n",
      " - 1s - loss: 0.5594 - val_loss: 11.4815\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 8.03424\n",
      "Epoch 90/100\n",
      " - 1s - loss: 0.5641 - val_loss: 11.5889\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 8.03424\n",
      "Epoch 91/100\n",
      " - 1s - loss: 0.5494 - val_loss: 11.6564\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 8.03424\n",
      "Epoch 92/100\n",
      " - 1s - loss: 0.5384 - val_loss: 11.6734\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 8.03424\n",
      "Epoch 93/100\n",
      " - 1s - loss: 0.5011 - val_loss: 11.6537\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 8.03424\n",
      "Epoch 94/100\n",
      " - 1s - loss: 0.5546 - val_loss: 11.6268\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 8.03424\n",
      "Epoch 95/100\n",
      " - 1s - loss: 0.5153 - val_loss: 11.5736\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 8.03424\n",
      "Epoch 96/100\n",
      " - 1s - loss: 0.4532 - val_loss: 11.5833\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 8.03424\n",
      "Epoch 97/100\n",
      " - 1s - loss: 0.4657 - val_loss: 11.5837\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 8.03424\n",
      "Epoch 98/100\n",
      " - 1s - loss: 0.4736 - val_loss: 11.6315\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 8.03424\n",
      "Epoch 99/100\n",
      " - 1s - loss: 0.4817 - val_loss: 11.6894\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 8.03424\n",
      "Epoch 100/100\n",
      " - 1s - loss: 0.4452 - val_loss: 11.7813\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 8.03424\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5bb895c8d0>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([X1train, X2train], ytrain, epochs=100, verbose=2, callbacks=[checkpoint], validation_data=([X1test, X2test], ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_generator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-109-81c04808731d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;31m# create the data generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mgenerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_descriptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0;31m# fit for one epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_generator' is not defined"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "steps = len(train_descriptions)\n",
    "for i in range(epochs):\n",
    "\t# create the data generator\n",
    "\tgenerator = data_generator(train_descriptions, train_features, tokenizer, max_length)\n",
    "\t# fit for one epoch\n",
    "\tmodel.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n",
    "\t# save model\n",
    "\tmodel.save('model_' + str(i) + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'tokenizer.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-108-ed3a0c38012d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizer.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# pre-define the max sequence length (from training)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmax_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m34\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'tokenizer.pkl'"
     ]
    }
   ],
   "source": [
    "tokenizer = load(open('tokenizer.pkl', 'rb'))\n",
    "# pre-define the max sequence length (from training)\n",
    "max_length = 34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model-ep020-loss3.010-val_loss=8.057.h5'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'model-ep020-loss3.010-val_loss=8.057.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
